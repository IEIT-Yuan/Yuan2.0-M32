




### <font color=#FFC125 >æº2.0-MoE æ¨¡å‹</font> 

-----
**ckptè½¬æ¢è¯´æ˜**
### <strong>ğŸ”˜ ckptè½¬æ¢</strong> 

æˆ‘ä»¬æä¾›çš„çš„æ¨¡å‹æ–‡ä»¶æ˜¯8è·¯æµæ°´å¹¶è¡Œï¼ˆ8ppï¼‰çš„æ¨¡å‹æ–‡ä»¶ï¼Œæˆ‘ä»¬æä¾›äº†è‡ªåŠ¨è½¬æ¢è„šæœ¬ï¼Œå¯ä»¥ä¾æ¬¡æ‰§è¡Œå®Œè½¬æ¢æµç¨‹ï¼Œä½¿ç”¨æ–¹å¼å¦‚ä¸‹ï¼š



**<font color=#FFFFF0 >å¦‚æœæå‰å°†8è·¯æµæ°´å¹¶è¡Œåˆå¹¶ï¼Œå¯ä»¥ç›´æ¥æ‰§è¡Œï¼š </font>**


```sh
bash examples/convert_hf_moe.sh
```

åœ¨è½¬æ¢æ—¶éœ€è¦


**<font color=#FFFFF0 >å¦‚æœä¸åˆå¹¶æµæ°´ï¼Œå¯ä»¥æŒ‰ä¸‹é¢çš„æ–¹å¼è¿›è¡Œè½¬æ¢ï¼š </font>**

é¦–å…ˆæ‰§è¡Œè½¬æ¢è„šæœ¬ï¼š

```sh
bash examples/convert_hf_moe.sh
```
æ‰§è¡Œè¿™ä¸ªè„šæœ¬ï¼Œæ¯ä¸€è·¯æµæ°´å¯¹åº”çš„.ckptæ–‡ä»¶éƒ½ä¼šç”Ÿæˆä¸€ä¸ªå¯¹åº”çš„.binæ–‡ä»¶ï¼Œç­‰å®Œæˆè½¬æ¢ä¹‹åå¯ä»¥åˆ é™¤è¿™äº›ä¸­é—´æ–‡ä»¶ã€‚

ç„¶åæ‰§è¡Œä¸‹é¢çš„å‘½ä»¤ï¼š

```sh
python tools/concat.py --input-path $input_path --output-path $output_path --pp_rank 8 --num_layers 24 
```

è¿™é‡Œçš„`--input-path`è®¾ç½®ä¸ºä¸Šä¸€æ­¥ä¸­äº§ç”Ÿçš„ä¸­é—´æ–‡ä»¶è·¯å¾„ï¼Œè¿™ä¸ªå‘½ä»¤ä¼šåœ¨`--output-path`è®¾ç½®çš„è·¯å¾„ä¸‹ç”Ÿæˆä¸€ä¸ªå®Œæ•´çš„.binæ–‡ä»¶ã€‚


### <strong>ğŸ”˜ binæ–‡ä»¶æ‹†åˆ†</strong> 
æ‰§è¡Œä¸Šé¢çš„è½¬æ¢å‘½ä»¤åä¼šç”Ÿæˆä¸€ä¸ªbinæ–‡ä»¶ï¼Œå¯ä»¥æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤å°†å…¶æ‹†åˆ†ï¼š
```sh
python tools/split_bin.py --input-path $input_path --output-path $output_path
```

### <strong>ğŸ”˜ HFæ¨¡å‹æ¨ç†</strong> 

å¯ä»¥é€šè¿‡å¦‚ä¸‹ä»£ç è°ƒç”¨YuanMoEæ¨¡å‹æ¥ç”Ÿæˆæ–‡æœ¬ï¼š 

```python
import torch, transformers
import sys, os
sys.path.append(
    os.path.abspath(os.path.join(os.path.dirname(__file__), os.path.pardir)))
from transformers import AutoModelForCausalLM,AutoTokenizer,LlamaTokenizer

print("Creat tokenizer...")
tokenizer = LlamaTokenizer.from_pretrained('IEITYuan/Yuan2-hf-moe', add_eos_token=False, add_bos_token=False, eos_token='<eod>')
tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)

print("Creat model...")
model = AutoModelForCausalLM.from_pretrained('IEITYuan/Yuan2-hf-moe', device_map='auto', torch_dtype=torch.bfloat16, trust_remote_code=True)

inputs = tokenizer("è¯·é—®ç›®å‰æœ€å…ˆè¿›çš„æœºå™¨å­¦ä¹ ç®—æ³•æœ‰å“ªäº›ï¼Ÿ", return_tensors="pt")["input_ids"].to("cuda:0")
outputs = model.generate(inputs,do_sample=False,max_length=100)
print(tokenizer.decode(outputs[0]))
```
